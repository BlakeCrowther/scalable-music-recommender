{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Suppress native-hadoop warning\n",
        "!sed -i '$a\\# Add the line for suppressing the NativeCodeLoader warning \\nlog4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR,console' /$HADOOP_HOME/etc/hadoop/log4j.properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_dir = os.path.join(os.getcwd(), 'work')\n",
        "data_dir = os.path.join(base_dir, 'data')\n",
        "\n",
        "# Function to load a specified dataset to HDFS\n",
        "def load_to_hdfs(dataset):\n",
        "    path = os.path.join(data_dir, dataset)\n",
        "    hdfs_path = f'/data/{dataset}'\n",
        "    hdfs_dir = os.path.dirname(hdfs_path)\n",
        "    \n",
        "    try:\n",
        "        if hdfs_dir:\n",
        "            subprocess.run(['hadoop', 'fs', '-mkdir', '-p', hdfs_dir], check=True, capture_output=True)\n",
        "        \n",
        "        if os.path.isdir(path):\n",
        "            # Copy directory recursively\n",
        "            subprocess.run(['hadoop', 'fs', '-put', '-f', path, hdfs_dir], check=True, capture_output=True)\n",
        "        else:\n",
        "            # Copy single file\n",
        "            subprocess.run(['hadoop', 'fs', '-put', '-f', path, hdfs_path], check=True, capture_output=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error uploading {dataset} to HDFS\")\n",
        "        print(\"Command:\", e.cmd)\n",
        "        print(\"Return code:\", e.returncode)\n",
        "        print(\"Output:\", e.output.decode())\n",
        "        print(\"Error:\", e.stderr.decode())\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted /data\n"
          ]
        }
      ],
      "source": [
        "datasets = [\n",
        "    'raw/train',\n",
        "    'raw/test',\n",
        "    'processed/user_rating_downsampled.txt',\n",
        "    'processed/user_and_song_rating_downsampled.txt',\n",
        "    'song-attributes.txt',\n",
        "    'genre-hierarchy.txt',\n",
        "]\n",
        "\n",
        "# Clean up the existing data\n",
        "subprocess.run(['hdfs', 'dfs', '-rm', '-r', f'/data'])\n",
        "subprocess.run(['hdfs', 'dfs', '-mkdir', '-p', '/data'])\n",
        "\n",
        "for dataset in datasets:\n",
        "    load_to_hdfs(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Listing directories in HDFS\n",
            "drwxr-xr-x   - root supergroup          0 2024-06-04 20:49 /data\n",
            "-rw-r--r--   1 root supergroup       4362 2024-06-04 20:49 /data/genre-hierarchy.txt\n",
            "drwxr-xr-x   - root supergroup          0 2024-06-04 20:49 /data/processed\n",
            "-rw-r--r--   1 root supergroup  409379887 2024-06-04 20:49 /data/processed/user_and_song_rating_downsampled.txt\n",
            "-rw-r--r--   1 root supergroup  763640390 2024-06-04 20:49 /data/processed/user_rating_downsampled.txt\n",
            "drwxr-xr-x   - root supergroup          0 2024-06-04 20:49 /data/raw\n",
            "drwxr-xr-x   - root supergroup          0 2024-06-04 20:49 /data/raw/test\n",
            "-rw-r--r--   1 root supergroup   29250779 2024-06-04 20:49 /data/raw/test/test_0.txt\n",
            "-rw-r--r--   1 root supergroup   30361653 2024-06-04 20:49 /data/raw/test/test_1.txt\n",
            "-rw-r--r--   1 root supergroup   30360260 2024-06-04 20:49 /data/raw/test/test_2.txt\n",
            "-rw-r--r--   1 root supergroup   30360544 2024-06-04 20:49 /data/raw/test/test_3.txt\n",
            "-rw-r--r--   1 root supergroup   30360882 2024-06-04 20:49 /data/raw/test/test_4.txt\n",
            "-rw-r--r--   1 root supergroup   32362435 2024-06-04 20:49 /data/raw/test/test_5.txt\n",
            "-rw-r--r--   1 root supergroup   32362727 2024-06-04 20:49 /data/raw/test/test_6.txt\n",
            "-rw-r--r--   1 root supergroup   32360794 2024-06-04 20:49 /data/raw/test/test_7.txt\n",
            "-rw-r--r--   1 root supergroup   32360897 2024-06-04 20:49 /data/raw/test/test_8.txt\n",
            "-rw-r--r--   1 root supergroup    3750961 2024-06-04 20:49 /data/raw/test/test_9.txt\n",
            "drwxr-xr-x   - root supergroup          0 2024-06-04 20:49 /data/raw/train\n",
            "-rw-r--r--   1 root supergroup 1116948159 2024-06-04 20:49 /data/raw/train/train_0.txt\n",
            "-rw-r--r--   1 root supergroup 1165261648 2024-06-04 20:49 /data/raw/train/train_1.txt\n",
            "-rw-r--r--   1 root supergroup 1167807043 2024-06-04 20:49 /data/raw/train/train_2.txt\n",
            "-rw-r--r--   1 root supergroup 1168046738 2024-06-04 20:49 /data/raw/train/train_3.txt\n",
            "-rw-r--r--   1 root supergroup 1167900398 2024-06-04 20:48 /data/raw/train/train_4.txt\n",
            "-rw-r--r--   1 root supergroup 1243252130 2024-06-04 20:49 /data/raw/train/train_5.txt\n",
            "-rw-r--r--   1 root supergroup 1241075737 2024-06-04 20:48 /data/raw/train/train_6.txt\n",
            "-rw-r--r--   1 root supergroup 1241481728 2024-06-04 20:48 /data/raw/train/train_7.txt\n",
            "-rw-r--r--   1 root supergroup 1241984479 2024-06-04 20:48 /data/raw/train/train_8.txt\n",
            "-rw-r--r--   1 root supergroup  143179231 2024-06-04 20:48 /data/raw/train/train_9.txt\n",
            "-rw-r--r--   1 root supergroup    2567765 2024-06-04 20:49 /data/song-attributes.txt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['hdfs', 'dfs', '-ls', '-R', '/'], returncode=0)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# list directories in hdfs for users\n",
        "print(f'\\nListing directories in HDFS')\n",
        "# crawl all of hdfs and list all directories\n",
        "subprocess.run(['hdfs', 'dfs', '-ls', '-R', '/'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
