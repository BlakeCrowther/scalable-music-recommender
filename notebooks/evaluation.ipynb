{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress native-hadoop warning\n",
    "!sed -i '$a\\# Add the line for suppressing the NativeCodeLoader warning \\nlog4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR,console' /$HADOOP_HOME/etc/hadoop/log4j.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/work')\n",
    "\n",
    "BASE_DIR = '/home/work'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from data.utils.data_loader import load_from_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.app.submitTime', '1715811788579')\n",
      "('spark.app.id', 'local-1715811789294')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.host', '50278eaf4c0f')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.driver.port', '39423')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.name', 'MusicRecommender')\n",
      "('spark.driver.memory', '14g')\n",
      "('spark.app.startTime', '1715811788729')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n"
     ]
    }
   ],
   "source": [
    "# Set Spark Settings\n",
    "conf = pyspark.SparkConf().setAll([\n",
    "    ('spark.master', 'local[*]'),\n",
    "    ('spark.app.name', 'MusicRecommender'),\n",
    "    # ('spark.executor.instances', '2'),  # Number of executors\n",
    "    # ('spark.executor.cores', '8'),  # Cores per executor\n",
    "    # ('spark.executor.memory', '10g'),  # Memory per executor\n",
    "    ('spark.driver.memory','14g'),\n",
    "])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Print Spark Settings\n",
    "settings = spark.sparkContext.getConf().getAll()\n",
    "for s in settings:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load Test Data\n",
    "_, test_data = load_from_hdfs('raw', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.154756499116459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load Models\n",
    "als_model = ALSModel.load(f'file://{BASE_DIR}/models/als_model')\n",
    "\n",
    "# Define Evaluators\n",
    "rmse = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "# Make Predictions\n",
    "predictions = als_model.transform(test_data)\n",
    "\n",
    "# Evaluate Model\n",
    "rmse_val = rmse.evaluate(predictions)\n",
    "\n",
    "# Print Evaluation Metrics\n",
    "print(f'RMSE: {rmse_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+------------+----------+\n",
      "|user_id|song_id|rating|partition_id|prediction|\n",
      "+-------+-------+------+------------+----------+\n",
      "|      0|   7171|     5|           0|  4.207943|\n",
      "|      0|   8637|     4|           0| 4.4320993|\n",
      "|      0|  21966|     4|           0| 4.1567163|\n",
      "|      0|  35821|     5|           0|  4.286071|\n",
      "|      0|  82446|     5|           0| 4.6395755|\n",
      "|      0|  90409|     5|           0|  4.163314|\n",
      "|      0| 107410|     5|           0|  4.401118|\n",
      "|      0| 131919|     5|           0| 4.8896813|\n",
      "|      0| 132685|     3|           0| 4.2512097|\n",
      "|      0| 136507|     3|           0| 4.1017985|\n",
      "|      1|   3342|     5|           0| 3.9412968|\n",
      "|      1|   7522|     1|           0| 2.1048512|\n",
      "|      1|  25363|     2|           0| 3.2813597|\n",
      "|      1|  38997|     5|           0| 3.6586933|\n",
      "|      1|  43685|     1|           0| 1.5671213|\n",
      "|      1|  45488|     4|           0| 3.7472024|\n",
      "|      1|  62770|     3|           0| 3.1563296|\n",
      "|      1| 109450|     3|           0| 3.6827579|\n",
      "|      1| 122506|     3|           0| 2.7749026|\n",
      "|      1| 130621|     5|           0| 3.6269293|\n",
      "+-------+-------+------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show Predictions\n",
    "predictions.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
