{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress native-hadoop warning\n",
    "!sed -i '$a\\# Add the line for suppressing the NativeCodeLoader warning \\nlog4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR,console' /$HADOOP_HOME/etc/hadoop/log4j.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/work')\n",
    "\n",
    "BASE_DIR = '/home/work'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "from data.utils import load_from_hdfs, user_based_train_test_split, load_data\n",
    "from models.utils import load_model\n",
    "from models.evaluation_metrics import calculate_rmse, calculate_mae, calculate_song_coverage, calculate_user_coverage, calculate_precision_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.master', 'local[3]')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.host', '693f94dcf7da')\n",
      "('spark.app.startTime', '1717134134288')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.driver.port', '43315')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.app.id', 'local-1717134134940')\n",
      "('spark.app.submitTime', '1717134134200')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.name', 'MusicRecommender')\n",
      "('spark.driver.memory', '14g')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n"
     ]
    }
   ],
   "source": [
    "# Set Spark Settings\n",
    "conf = pyspark.SparkConf().setAll([\n",
    "    ('spark.master', 'local[3]'),\n",
    "    ('spark.app.name', 'MusicRecommender'),\n",
    "    ('spark.driver.memory','14g'),\n",
    "    # ('spark.sql.shuffle.partitions', '200'),\n",
    "])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Print Spark Settings\n",
    "settings = spark.sparkContext.getConf().getAll()\n",
    "for s in settings:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Variables\n",
    "test_user_ratings = 10\n",
    "data_scale = 0.2\n",
    "seed = 42\n",
    "\n",
    "# Results DataFrame\n",
    "results = []\n",
    "model_dirs = ['als_model/raw', 'als_model/user_rating_downsampled', 'als_model/user_song_rating_downsampled']\n",
    "model_type = 'ALS'\n",
    "\n",
    "# Load Datasets\n",
    "raw_train, raw_test = load_from_hdfs('raw', 2)\n",
    "combined_raw = raw_train.union(raw_test)\n",
    "\n",
    "user_rating_downsampled = load_data('processed/user_rating_downsampled.txt')\n",
    "user_song_rating_downsampled = load_data('processed/user_and_song_rating_downsampled.txt')\n",
    "\n",
    "datasets = [combined_raw, user_rating_downsampled, user_song_rating_downsampled]\n",
    "split_datasets = [user_based_train_test_split(dataset, test_user_ratings, data_scale, seed) for dataset in datasets]\n",
    "model_names = ['raw', 'user_rating_downsampled', 'user_song_rating_downsampled']\n",
    "\n",
    "# Static Variables\n",
    "total_size = 717872016\n",
    "total_users = 1823179\n",
    "total_songs = 136736\n",
    "\n",
    "user_recs = None\n",
    "predictions = None\n",
    "\n",
    "batch_size = 10000\n",
    "n_recs = 10\n",
    "\n",
    "for model_dir, model_name, dataset in zip(model_dirs, model_names, split_datasets):\n",
    "    train_data, test_data = dataset\n",
    "    # Clear Cache\n",
    "    spark.catalog.clearCache()\n",
    "            \n",
    "    # Load Model\n",
    "    model_path = f'file://{BASE_DIR}/models/{model_dir}'\n",
    "    model = load_model(model_type, model_path)\n",
    "    \n",
    "    # Train Metrics\n",
    "    train_user_ids = train_data.select('user_id').distinct()\n",
    "    train_users = train_user_ids.count()\n",
    "    train_song_ids = train_data.select('song_id').distinct()\n",
    "    train_songs = train_song_ids.count()\n",
    "    \n",
    "    # Song Metrics\n",
    "    test_user_ids = test_data.select('user_id').distinct()\n",
    "    test_users = test_user_ids.count()\n",
    "    test_song_ids = test_data.select('song_id').distinct()\n",
    "    test_songs = test_song_ids.count()\n",
    "\n",
    "    spark.catalog.clearCache()\n",
    "    \n",
    "    # Recommendation Metrics\n",
    "    user_recs = model.recommendForAllUsers(n_recs)\n",
    "    user_recs = user_recs.select('user_id', F.explode('recommendations').alias('recommendation'))\n",
    "    user_recs = user_recs.select('user_id', F.col('recommendation.song_id').alias('song_id'), F.col('recommendation.rating').alias('rating'))\n",
    "                \n",
    "    recommendation_users = user_recs.select('user_id').distinct().count()\n",
    "    recommendation_songs = user_recs.select('song_id').distinct().count()\n",
    "    \n",
    "                            \n",
    "    # Coverage Metrics\n",
    "    test_song_coverage = calculate_song_coverage(train_songs, test_songs)\n",
    "    test_overall_song_coverage = calculate_song_coverage(total_songs, test_songs)\n",
    "    test_user_coverage = calculate_user_coverage(train_users, test_users)\n",
    "    test_overall_user_coverage = calculate_user_coverage(total_users, test_users)\n",
    "    recommendations_song_coverage = calculate_song_coverage(train_songs, recommendation_songs)\n",
    "    recommendations_overall_song_coverage = calculate_song_coverage(total_songs, recommendation_songs)\n",
    "    recommendations_user_coverage = calculate_user_coverage(train_users, recommendation_users)\n",
    "    recommendations_overall_user_coverage = calculate_user_coverage(total_users, recommendation_users)\n",
    "    \n",
    "    # Get Predictions\n",
    "    predictions = model.transform(test_data)\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    rmse = calculate_rmse(predictions)\n",
    "    mae = calculate_mae(predictions)\n",
    "    precision, recall = calculate_precision_recall(user_recs, test_data, 0.0)            \n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Dataset': dataset,\n",
    "        'Users(Train:Test)': f'{train_users} : {test_users}',\n",
    "        'Songs(Train:Test)': f'{train_songs} : {test_songs}',\n",
    "        'Test User Coverage(Model:Overall)': f'{round(test_user_coverage, 2)} : {round(test_overall_user_coverage, 2)}',\n",
    "        'Test Song Coverage(Model:Overall)': f'{round(test_song_coverage, 2)} : {round(test_overall_song_coverage, 2)}',\n",
    "        'Recommendations User Coverage(Model:Overall)': f'{round(recommendations_user_coverage, 2)} : {round(recommendations_overall_user_coverage, 2)}',\n",
    "        'Recommendations Song Coverage(Model:Overall)': f'{round(recommendations_song_coverage, 2)} : {round(recommendations_overall_song_coverage, 2)}',\n",
    "        'Recommendations Precision': round(precision, 4),\n",
    "        'Recommendations Recall': round(recall, 4),     \n",
    "        'Predictions RMSE': rmse,\n",
    "        'Predictions MAE': mae,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded partition 0: 11650865 training records and 1922481 test records from HDFS\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- song_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- partition_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results DataFrame\n",
    "model_dir = 'als_model/user_song_balanced'\n",
    "model_type = 'ALS'\n",
    "dataset = 'processed/user_song_balanced'\n",
    "par = 0\n",
    "\n",
    "# Static Variables\n",
    "total_size = 717872016\n",
    "total_users = 1823179\n",
    "total_songs = 136736\n",
    "\n",
    "n_recs = 10\n",
    "\n",
    "# Clear Cache\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Load Data\n",
    "train_data, test_data = load_from_hdfs(dataset, par)\n",
    "\n",
    "# Load Model\n",
    "model_path = f'file://{BASE_DIR}/models/{model_dir}'\n",
    "model = load_model(model_type, model_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train Metrics\n",
    "# train_user_ids = train_data.select('user_id').distinct()\n",
    "# train_users = train_user_ids.count()\n",
    "# train_song_ids = train_data.select('song_id').distinct()\n",
    "# train_songs = train_song_ids.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Song Metrics\n",
    "# test_user_ids = test_data.select('user_id').distinct()\n",
    "# test_users = test_user_ids.count()\n",
    "# test_song_ids = test_data.select('song_id').distinct()\n",
    "# test_songs = test_song_ids.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"'Users(Train:Test)': {train_users} : {test_users}\")\n",
    "# print(f\"'Songs(Train:Test)': {train_songs} : {test_songs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Recommendation Metrics\n",
    "user_recs = model.recommendForAllUsers(n_recs)\n",
    "user_recs = user_recs.select('user_id', F.explode('recommendations').alias('recommendation'))\n",
    "user_recs = user_recs.select('user_id', F.col('recommendation.song_id').alias('song_id'), F.col('recommendation.rating').alias('rating'))\n",
    "\n",
    "recommendation_users = user_recs.select('user_id').distinct()\n",
    "recommendation_songs = user_recs.count()\n",
    "recommendation_songs = user_recs.select('song_id').distinct()\n",
    "recommendation_songs = user_recs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Coverage Metrics\n",
    "# test_song_coverage = calculate_song_coverage(train_songs, test_songs)\n",
    "# test_overall_song_coverage = calculate_song_coverage(total_songs, test_songs)\n",
    "# test_user_coverage = calculate_user_coverage(train_users, test_users)\n",
    "# test_overall_user_coverage = calculate_user_coverage(total_users, test_users)\n",
    "# recommendations_song_coverage = calculate_song_coverage(train_songs, recommendation_songs)\n",
    "# recommendations_overall_song_coverage = calculate_song_coverage(total_songs, recommendation_songs)\n",
    "# recommendations_user_coverage = calculate_user_coverage(train_users, recommendation_users)\n",
    "# recommendations_overall_user_coverage = calculate_user_coverage(total_users, recommendation_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"'Test User Coverage(Model:Overall)': {round(test_user_coverage, 2)} : {round(test_overall_user_coverage, 2)}\")\n",
    "# print(f\"'Test Song Coverage(Model:Overall)': {round(test_song_coverage, 2)} : {round(test_overall_song_coverage, 2)}\")\n",
    "# print(f\"'Recommendations User Coverage(Model:Overall)': {round(recommendations_user_coverage, 2)} : {round(recommendations_overall_user_coverage, 2)}\")\n",
    "# print(f\"'Recommendations Song Coverage(Model:Overall)': {round(recommendations_song_coverage, 2)} : {round(recommendations_overall_song_coverage, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Predictions\n",
    "spark.catalog.clearCache()\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0005877247224475671 = Relevant Recommendations: 1036 / Total Recommendations: 1762730\n",
      "Recall: 0.0005388869903005544 = Relevant Recommendations: 1036 / Relevant Test Items: 1922481\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation Metrics\n",
    "rmse = calculate_rmse(predictions)\n",
    "mae = calculate_mae(predictions)\n",
    "precision, recall = calculate_precision_recall(user_recs, test_data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Recommendations Precision': 0.0006\n",
      "'Recommendations Recall': 0.0005\n",
      "'Predictions RMSE': 1.0726797541719026\n",
      "'Predictions MAE': 0.8087341378166875\n"
     ]
    }
   ],
   "source": [
    "print(f\"'Recommendations Precision': {round(precision, 4)}\")\n",
    "print(f\"'Recommendations Recall': {round(recall, 4)}\")\n",
    "print(f\"'Predictions RMSE': {rmse}\")\n",
    "print(f\"'Predictions MAE': {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation Results Schema\n",
    "# schema = StructType([\n",
    "#     StructField(\"Model\", StringType(), True),\n",
    "#     StructField(\"Dataset\", StringType(), True),\n",
    "#     StructField(\"Users(Train:Test)\", StringType(), True),\n",
    "#     StructField(\"Songs(Train:Test)\", StringType(), True),\n",
    "#     StructField(\"Test User Coverage(Model:Overall)\", StringType(), True),\n",
    "#     StructField(\"Test Song Coverage(Model:Overall)\", StringType(), True),\n",
    "#     StructField(\"Recommendations User Coverage(Model:Overall)\", StringType(), True),\n",
    "#     StructField(\"Recommendations Song Coverage(Model:Overall)\", StringType(), True),\n",
    "#     StructField(\"Recommendations Precision\", FloatType(), True),\n",
    "#     StructField(\"Recommendations Recall\", FloatType(), True),\n",
    "#     StructField(\"Predictions RMSE\", FloatType(), True),\n",
    "#     StructField(\"Predictions MAE\", FloatType(), True),\n",
    "# ])\n",
    "\n",
    "# # Output Results as DF\n",
    "# results_df = spark.createDataFrame(results, schema)\n",
    "# results_df.show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data points for three datasets\n",
    "splits = ['Raw', 'Downscaled User Ratings', 'Downscaled User & Song Ratings']\n",
    "precision = [0.0005, 0.0002, 0.0006]\n",
    "recall = [0.0005 , 0.0001, 0.0005]\n",
    "rmse = [1.119, 1.031, 1.072]\n",
    "mae = [0.884, 0.809, 0.808]\n",
    "\n",
    "# Creating the bar width\n",
    "bar_width = 0.2\n",
    "\n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(splits))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "r3 = [x + bar_width for x in r2]\n",
    "r4 = [x + bar_width for x in r3]\n",
    "\n",
    "# Plotting the bar chart with a logarithmic scale\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.bar(r1, precision, color='blue', width=bar_width, edgecolor='grey', label='Precision')\n",
    "plt.bar(r2, recall, color='orange', width=bar_width, edgecolor='grey', label='Recall')\n",
    "plt.bar(r3, rmse, color='green', width=bar_width, edgecolor='grey', label='RMSE')\n",
    "plt.bar(r4, mae, color='red', width=bar_width, edgecolor='grey', label='MAE')\n",
    "\n",
    "# Adding labels, title, and legend\n",
    "plt.xlabel('Splits', fontweight='bold')\n",
    "plt.xticks([r + bar_width for r in range(len(splits))], splits)\n",
    "plt.ylabel('Values (Log Scale)', fontweight='bold')\n",
    "plt.yscale('log')  # Set y-axis to logarithmic scale\n",
    "plt.title('Model Evaluation Metrics Across Splits')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
