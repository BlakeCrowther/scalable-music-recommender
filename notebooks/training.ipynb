{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress native-hadoop warning\n",
    "!sed -i '$a\\# Add the line for suppressing the NativeCodeLoader warning \\nlog4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR,console' /$HADOOP_HOME/etc/hadoop/log4j.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/work')\n",
    "\n",
    "BASE_DIR = '/home/work'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from data.utils.data_loader import load_from_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.app.startTime', '1715810974721')\n",
      "('spark.driver.port', '33083')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.host', '50278eaf4c0f')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.app.id', 'local-1715810975738')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.submitTime', '1715810974566')\n",
      "('spark.app.name', 'MusicRecommender')\n",
      "('spark.driver.memory', '14g')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n"
     ]
    }
   ],
   "source": [
    "# Set Spark Settings\n",
    "conf = pyspark.SparkConf().setAll([\n",
    "    ('spark.master', 'local[*]'),\n",
    "    ('spark.app.name', 'MusicRecommender'),\n",
    "    # ('spark.executor.instances', '2'),  # Number of executors\n",
    "    # ('spark.executor.cores', '8'),  # Cores per executor\n",
    "    # ('spark.executor.memory', '10g'),  # Memory per executor\n",
    "    ('spark.driver.memory','14g'),\n",
    "])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Print Spark Settings\n",
    "settings = spark.sparkContext.getConf().getAll()\n",
    "for s in settings:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"raw\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data, test_data = load_from_hdfs('raw', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size:  76344627\n",
      "+-------+-------+------+------------+\n",
      "|user_id|song_id|rating|partition_id|\n",
      "+-------+-------+------+------------+\n",
      "|      0|    166|     5|           0|\n",
      "|      0|   2245|     4|           0|\n",
      "|      0|   3637|     4|           0|\n",
      "|      0|   5580|     4|           0|\n",
      "|      0|   5859|     4|           0|\n",
      "+-------+-------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data size: \", train_data.count())\n",
    "# print head\n",
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- song_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- partition_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ALS model\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"song_id\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "# als = ALS(userCol=\"user_id\", itemCol=\"song_id\", ratingCol=\"rating\", coldStartStrategy=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune model hyperparameters\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.maxIter, [5]) \\\n",
    "    .addGrid(als.regParam, [0.1]) \\\n",
    "    .addGrid(als.rank, [5]) \\\n",
    "    .build()\n",
    "    \n",
    "# Define a model evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# Build cross validator\n",
    "tvs = TrainValidationSplit(estimator=als,\n",
    "                            estimatorParamMaps=param_grid,\n",
    "                            evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: ALSModel: uid=ALS_bd27ca2bffba, rank=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train ALS model\n",
    "try:  \n",
    "    # clear spark cache to avoid memory issues\n",
    "    spark.catalog.clearCache()\n",
    "\n",
    "    # Fit ALS model\n",
    "    model = tvs.fit(train_data)\n",
    "\n",
    "    # Get best model\n",
    "    best_model = model.bestModel\n",
    "    print(f'Best model: {best_model}')\n",
    "\n",
    "    # Save the best model for later evaluation\n",
    "    best_model.save(f'file://{BASE_DIR}/models/als_model')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'Error training ALS model: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
